{
  "worksheets": [
    {
      "cells": [
        {
          "metadata": {
            "id": "4embtkV0pNxM",
            "colab_type": "text"
          },
          "cell_type": "markdown",
          "source": "Deep Learning\n=============\n\nAssignment 4\n------------\n\nPreviously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n\nThe goal of this assignment is make the neural network convolutional."
        },
        {
          "metadata": {
            "id": "tm2CQN_Cpwj0",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              }
            },
            "cellView": "both"
          },
          "cell_type": "code",
          "input": "# These are all the modules we'll be using later. Make sure you can import them\n# before proceeding further.\nimport cPickle as pickle\nimport numpy as np\nimport tensorflow as tf",
          "language": "python",
          "outputs": []
        },
        {
          "metadata": {
            "id": "y3-cj1bpmuxc",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              },
              "output_extras": [
                {
                  "item_id": 1
                }
              ]
            },
            "cellView": "both",
            "executionInfo": {
              "elapsed": 11948,
              "status": "ok",
              "timestamp": 1446658914837,
              "user": {
                "color": "",
                "displayName": "",
                "isAnonymous": false,
                "isMe": true,
                "permissionId": "",
                "photoUrl": "",
                "sessionId": "0",
                "userId": ""
              },
              "user_tz": 480
            },
            "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
          },
          "cell_type": "code",
          "input": "pickle_file = 'notMNIST.pickle'\n\nwith open(pickle_file, 'rb') as f:\n  save = pickle.load(f)\n  train_dataset = save['train_dataset']\n  train_labels = save['train_labels']\n  valid_dataset = save['valid_dataset']\n  valid_labels = save['valid_labels']\n  test_dataset = save['test_dataset']\n  test_labels = save['test_labels']\n  del save  # hint to help gc free up memory\n  print 'Training set', train_dataset.shape, train_labels.shape\n  print 'Validation set', valid_dataset.shape, valid_labels.shape\n  print 'Test set', test_dataset.shape, test_labels.shape",
          "language": "python",
          "outputs": [
            {
              "output_type": "stream",
              "stream": "stdout",
              "text": "Training set (200000, 28, 28) (200000,)\nValidation set (10000, 28, 28) (10000,)\nTest set (18724, 28, 28) (18724,)\n"
            }
          ]
        },
        {
          "metadata": {
            "id": "L7aHrm6nGDMB",
            "colab_type": "text"
          },
          "cell_type": "markdown",
          "source": "Reformat into a TensorFlow-friendly shape:\n- convolutions need the image data formatted as a cube (width by height by #channels)\n- labels as float 1-hot encodings."
        },
        {
          "metadata": {
            "id": "IRSyYiIIGIzS",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              },
              "output_extras": [
                {
                  "item_id": 1
                }
              ]
            },
            "cellView": "both",
            "executionInfo": {
              "elapsed": 11952,
              "status": "ok",
              "timestamp": 1446658914857,
              "user": {
                "color": "",
                "displayName": "",
                "isAnonymous": false,
                "isMe": true,
                "permissionId": "",
                "photoUrl": "",
                "sessionId": "0",
                "userId": ""
              },
              "user_tz": 480
            },
            "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
          },
          "cell_type": "code",
          "input": "image_size = 28\nnum_labels = 10\nnum_channels = 1 # grayscale\n\nimport numpy as np\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape(\n    (-1, image_size, image_size, num_channels)).astype(np.float32)\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint 'Training set', train_dataset.shape, train_labels.shape\nprint 'Validation set', valid_dataset.shape, valid_labels.shape\nprint 'Test set', test_dataset.shape, test_labels.shape",
          "language": "python",
          "outputs": [
            {
              "output_type": "stream",
              "stream": "stdout",
              "text": "Training set (200000, 28, 28, 1) (200000, 10)\nValidation set (10000, 28, 28, 1) (10000, 10)\nTest set (18724, 28, 28, 1) (18724, 10)\n"
            }
          ]
        },
        {
          "metadata": {
            "id": "AgQDIREv02p1",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              }
            },
            "cellView": "both"
          },
          "cell_type": "code",
          "input": "def accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])",
          "language": "python",
          "outputs": []
        },
        {
          "metadata": {
            "id": "5rhgjmROXu2O",
            "colab_type": "text"
          },
          "cell_type": "markdown",
          "source": "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
        },
        {
          "metadata": {
            "id": "IZYv70SvvOan",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              }
            },
            "cellView": "both"
          },
          "cell_type": "code",
          "input": "batch_size = 16\npatch_size = 5\ndepth = 16\nnum_hidden = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n  # Input data.\n  tf_train_dataset = tf.placeholder(\n    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n  \n  # Variables.\n  layer1_weights = tf.Variable(tf.truncated_normal(\n      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n  layer1_biases = tf.Variable(tf.zeros([depth]))\n  layer2_weights = tf.Variable(tf.truncated_normal(\n      [patch_size, patch_size, depth, depth], stddev=0.1))\n  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n  layer3_weights = tf.Variable(tf.truncated_normal(\n      [image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n  layer4_weights = tf.Variable(tf.truncated_normal(\n      [num_hidden, num_labels], stddev=0.1))\n  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n  \n  # Model.\n  def model(data):\n    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n    hidden = tf.nn.relu(conv + layer1_biases)\n    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n    hidden = tf.nn.relu(conv + layer2_biases)\n    shape = hidden.get_shape().as_list()\n    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n    return tf.matmul(hidden, layer4_weights) + layer4_biases\n  \n  # Training computation.\n  logits = model(tf_train_dataset)\n  loss = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n    \n  # Optimizer.\n  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n  \n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(logits)\n  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n  test_prediction = tf.nn.softmax(model(tf_test_dataset))",
          "language": "python",
          "outputs": []
        },
        {
          "metadata": {
            "id": "noKFb2UovVFR",
            "colab_type": "code",
            "colab": {
              "autoexec": {
                "startup": false,
                "wait_interval": 0
              },
              "output_extras": [
                {
                  "item_id": 37
                }
              ]
            },
            "cellView": "both",
            "executionInfo": {
              "elapsed": 63292,
              "status": "ok",
              "timestamp": 1446658966251,
              "user": {
                "color": "",
                "displayName": "",
                "isAnonymous": false,
                "isMe": true,
                "permissionId": "",
                "photoUrl": "",
                "sessionId": "0",
                "userId": ""
              },
              "user_tz": 480
            },
            "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
          },
          "cell_type": "code",
          "input": "num_steps = 1001\n\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print \"Initialized\"\n  for step in xrange(num_steps):\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 50 == 0):\n      print \"Minibatch loss at step\", step, \":\", l\n      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n      print \"Validation accuracy: %.1f%%\" % accuracy(\n        valid_prediction.eval(), valid_labels)\n  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)",
          "language": "python",
          "outputs": [
            {
              "output_type": "stream",
              "stream": "stdout",
              "text": "Initialized\nMinibatch loss at step 0 : 3.51275\nMinibatch accuracy: 6.2%\nValidation accuracy: 12.8%\nMinibatch loss at step 50 : 1.48703\nMinibatch accuracy: 43.8%\nValidation accuracy: 50.4%\nMinibatch loss at step 100 : 1.04377\nMinibatch accuracy: 68.8%\nValidation accuracy: 67.4%\nMinibatch loss at step 150 : 0.601682\nMinibatch accuracy: 68.8%\nValidation accuracy: 73.0%\nMinibatch loss at step 200 : 0.898649\nMinibatch accuracy: 75.0%\nValidation accuracy: 77.8%\nMinibatch loss at step 250 : 1.3637\nMinibatch accuracy: 56.2%\nValidation accuracy: 75.4%\nMinibatch loss at step 300 : 1.41968\nMinibatch accuracy: 62.5%\nValidation accuracy: 76.0%\nMinibatch loss at step 350 : 0.300648\nMinibatch accuracy: 81.2%\nValidation accuracy: 80.2%\nMinibatch loss at step 400 : 1.32092\nMinibatch accuracy: 56.2%\nValidation accuracy: 80.4%\nMinibatch loss at step 450 : 0.556701\nMinibatch accuracy: 81.2%\nValidation accuracy: 79.4%\nMinibatch loss at step 500 : 1.65595\nMinibatch accuracy: 43.8%\nValidation accuracy: 79.6%\nMinibatch loss at step 550 : 1.06995\nMinibatch accuracy: 75.0%\nValidation accuracy: 81.2%\nMinibatch loss at step 600 : 0.223684\nMinibatch accuracy: 100.0%\nValidation accuracy: 82.3%\nMinibatch loss at step 650 : 0.619602\nMinibatch accuracy: 87.5%\nValidation accuracy: 81.8%\nMinibatch loss at step 700 : 0.812091\nMinibatch accuracy: 75.0%\nValidation accuracy: 82.4%\nMinibatch loss at step 750 : 0.276302\nMinibatch accuracy: 87.5%\nValidation accuracy: 82.3%\nMinibatch loss at step 800 : 0.450241\nMinibatch accuracy: 81.2%\nValidation accuracy: 82.3%\nMinibatch loss at step 850 : 0.137139\nMinibatch accuracy: 93.8%\nValidation accuracy: 82.3%\nMinibatch loss at step 900 : 0.52664\nMinibatch accuracy: 75.0%\nValidation accuracy: 82.2%\nMinibatch loss at step 950 : 0.623835\nMinibatch accuracy: 87.5%\nValidation accuracy: 82.1%\nMinibatch loss at step 1000 : 0.243114\nMinibatch accuracy: 93.8%\nValidation accuracy: 82.9%\nTest accuracy: 90.0%\n"
            }
          ]
        },
        {
          "metadata": {
            "id": "KedKkn4EutIK",
            "colab_type": "text"
          },
          "cell_type": "markdown",
          "source": "---\nProblem 1\n---------\n\nThe convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n\n---"
        },
        {
          "metadata": {
            "id": "klf21gpbAgb-",
            "colab_type": "text"
          },
          "cell_type": "markdown",
          "source": "---\nProblem 2\n---------\n\nTry to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n\n---"
        }
      ]
    }
  ],
  "metadata": {
    "name": "4_convolutions.ipynb",
    "colabVersion": "0.3.2",
    "colab_views": {},
    "colab_default_view": {}
  },
  "nbformat": 3,
  "nbformat_minor": 0
}
